{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "base_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7'\n",
    "}\n",
    "\n",
    "\n",
    "def get_page(url, options={}):\n",
    "    \"\"\"\n",
    "    抓取代理\n",
    "    :param url:\n",
    "    :param options:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    headers = dict(base_headers, **options)\n",
    "    print('正在抓取', url)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print('抓取成功', url, response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "    except ConnectionError:\n",
    "        print('抓取失败', url)\n",
    "        return None\n",
    "    \n",
    "\n",
    "import json\n",
    "import re\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "\n",
    "class ProxyMetaclass(type):\n",
    "    def __new__(cls, name, bases, attrs):\n",
    "        count = 0\n",
    "        attrs['__CrawlFunc__'] = []\n",
    "        for k, v in attrs.items():\n",
    "            if 'crawl_' in k:\n",
    "                attrs['__CrawlFunc__'].append(k)\n",
    "                count += 1\n",
    "        attrs['__CrawlFuncCount__'] = count\n",
    "        return type.__new__(cls, name, bases, attrs)\n",
    "\n",
    "\n",
    "class Crawler(object, metaclass=ProxyMetaclass):\n",
    "    def get_proxies(self, callback):\n",
    "        proxies = []\n",
    "        for proxy in eval(\"self.{}()\".format(callback)):\n",
    "            print('成功获取到代理', proxy)\n",
    "            proxies.append(proxy)\n",
    "        return proxies\n",
    "\n",
    "    def crawl_daili66(self, page_count=4):\n",
    "        \"\"\"\n",
    "        获取代理66\n",
    "        :param page_count: 页码\n",
    "        :return: 代理\n",
    "        \"\"\"\n",
    "        start_url = 'http://www.66ip.cn/{}.html'\n",
    "        urls = [start_url.format(page) for page in range(1, page_count + 1)]\n",
    "        for url in urls:\n",
    "            print('Crawling', url)\n",
    "            html = get_page(url)\n",
    "            if html:\n",
    "                doc = pq(html)\n",
    "                trs = doc('.containerbox table tr:gt(0)').items()\n",
    "                for tr in trs:\n",
    "                    ip = tr.find('td:nth-child(1)').text()\n",
    "                    port = tr.find('td:nth-child(2)').text()\n",
    "                    yield ':'.join([ip, port])\n",
    "\n",
    "    def crawl_proxy360(self):\n",
    "        \"\"\"\n",
    "        获取Proxy360\n",
    "        :return: 代理\n",
    "        \"\"\"\n",
    "        start_url = 'http://www.proxy360.cn/Region/China'\n",
    "        print('Crawling', start_url)\n",
    "        html = get_page(start_url)\n",
    "        if html:\n",
    "            doc = pq(html)\n",
    "            lines = doc('div[name=\"list_proxy_ip\"]').items()\n",
    "            for line in lines:\n",
    "                ip = line.find('.tbBottomLine:nth-child(1)').text()\n",
    "                port = line.find('.tbBottomLine:nth-child(2)').text()\n",
    "                yield ':'.join([ip, port])\n",
    "\n",
    "    def crawl_goubanjia(self):\n",
    "        \"\"\"\n",
    "        获取Goubanjia\n",
    "        :return: 代理\n",
    "        \"\"\"\n",
    "        start_url = 'http://www.goubanjia.com/free/gngn/index.shtml'\n",
    "        html = get_page(start_url)\n",
    "        if html:\n",
    "            doc = pq(html)\n",
    "            tds = doc('td.ip').items()\n",
    "            for td in tds:\n",
    "                td.find('p').remove()\n",
    "                yield td.text().replace(' ', '')\n",
    "\n",
    "    def crawl_ip181(self):\n",
    "        start_url = 'http://www.ip181.com/'\n",
    "        html = get_page(start_url)\n",
    "        ip_address = re.compile('<tr.*?>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "        # \\s* 匹配空格，起到换行作用\n",
    "        re_ip_address = ip_address.findall(html)\n",
    "        for address,port in re_ip_address:\n",
    "            result = address + ':' + port\n",
    "            yield result.replace(' ', '')\n",
    "\n",
    "\n",
    "    def crawl_ip3366(self):\n",
    "        for page in range(1, 4):\n",
    "            start_url = 'http://www.ip3366.net/free/?stype=1&page={}'.format(page)\n",
    "            html = get_page(start_url)\n",
    "            ip_address = re.compile('<tr>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "            # \\s * 匹配空格，起到换行作用\n",
    "            re_ip_address = ip_address.findall(html)\n",
    "            for address, port in re_ip_address:\n",
    "                result = address+':'+ port\n",
    "                yield result.replace(' ', '')\n",
    "\n",
    "\n",
    "    def crawl_kxdaili(self):\n",
    "        for i in range(1, 11):\n",
    "            start_url = 'http://www.kxdaili.com/ipList/{}.html#ip'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            ip_address = re.compile('<tr.*?>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "            # \\s* 匹配空格，起到换行作用\n",
    "            re_ip_address = ip_address.findall(html)\n",
    "            for address, port in re_ip_address:\n",
    "                result = address + ':' + port\n",
    "                yield result.replace(' ', '')\n",
    "\n",
    "\n",
    "    def crawl_premproxy(self):\n",
    "        for i in ['China-01','China-02','China-03','China-04','Taiwan-01']:\n",
    "            start_url = 'https://premproxy.com/proxy-by-country/{}.htm'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            if html:\n",
    "                ip_address = re.compile('<td data-label=\"IP:port \">(.*?)</td>') \n",
    "                re_ip_address = ip_address.findall(html)\n",
    "                for address_port in re_ip_address:\n",
    "                    yield address_port.replace(' ','')\n",
    "\n",
    "    def crawl_xroxy(self):\n",
    "        for i in ['CN','TW']:\n",
    "            start_url = 'http://www.xroxy.com/proxylist.php?country={}'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            if html:\n",
    "                ip_address1 = re.compile(\"title='View this Proxy details'>\\s*(.*).*\")\n",
    "                re_ip_address1 = ip_address1.findall(html)\n",
    "                ip_address2 = re.compile(\"title='Select proxies with port number .*'>(.*)</a>\") \n",
    "                re_ip_address2 = ip_address2.findall(html)\n",
    "                for address,port in zip(re_ip_address1,re_ip_address2):\n",
    "                    address_port = address+':'+port\n",
    "                    yield address_port.replace(' ','')\n",
    "    \n",
    "    def crawl_kuaidaili(self):\n",
    "        for i in range(1, 4):\n",
    "            start_url = 'http://www.kuaidaili.com/free/inha/{}/'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            if html:\n",
    "                ip_address = re.compile('<td data-title=\"IP\">(.*?)</td>') \n",
    "                re_ip_address = ip_address.findall(html)\n",
    "                port = re.compile('<td data-title=\"PORT\">(.*?)</td>')\n",
    "                re_port = port.findall(html)\n",
    "                for address,port in zip(re_ip_address, re_port):\n",
    "                    address_port = address+':'+port\n",
    "                    yield address_port.replace(' ','')\n",
    "    \n",
    "    def crawl_ip3366(self):\n",
    "        for i in range(1, 4):\n",
    "            start_url = 'http://www.ip3366.net/?stype=1&page={}'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            if html:\n",
    "                find_tr = re.compile('<tr>(.*?)</tr>', re.S)\n",
    "                trs = find_tr.findall(html)\n",
    "                for s in range(1, len(trs)):\n",
    "                    find_ip = re.compile('<td>(\\d+\\.\\d+\\.\\d+\\.\\d+)</td>')\n",
    "                    re_ip_address = find_ip.findall(trs[s])\n",
    "                    find_port = re.compile('<td>(\\d+)</td>')\n",
    "                    re_port = find_port.findall(trs[s])\n",
    "                    for address,port in zip(re_ip_address, re_port):\n",
    "                        address_port = address+':'+port\n",
    "                        yield address_port.replace(' ','')\n",
    "    \n",
    "    def crawl_iphai(self):\n",
    "        start_url = 'http://www.iphai.com/'\n",
    "        html = get_page(start_url)\n",
    "        if html:\n",
    "            find_tr = re.compile('<tr>(.*?)</tr>', re.S)\n",
    "            trs = find_tr.findall(html)\n",
    "            for s in range(1, len(trs)):\n",
    "                find_ip = re.compile('<td>\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+)\\s+</td>', re.S)\n",
    "                re_ip_address = find_ip.findall(trs[s])\n",
    "                find_port = re.compile('<td>\\s+(\\d+)\\s+</td>', re.S)\n",
    "                re_port = find_port.findall(trs[s])\n",
    "                for address,port in zip(re_ip_address, re_port):\n",
    "                    address_port = address+':'+port\n",
    "                    yield address_port.replace(' ','')\n",
    "\n",
    "    def crawl_89ip(self):\n",
    "        start_url = 'http://www.89ip.cn/apijk/?&tqsl=1000&sxa=&sxb=&tta=&ports=&ktip=&cf=1'\n",
    "        html = get_page(start_url)\n",
    "        if html:\n",
    "            find_ips = re.compile('(\\d+\\.\\d+\\.\\d+\\.\\d+:\\d+)', re.S)\n",
    "            ip_ports = find_ips.findall(html)\n",
    "            for address_port in ip_ports:\n",
    "                yield address_port\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
